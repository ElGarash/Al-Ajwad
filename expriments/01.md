# Experiment 1: Vanilla Whisper

_Tue Oct 24 04:51:01 AM EEST 2023_

Author: Omar Tarek

Signed-off: Abdulrhmn Ghanem

## Introduction:

Whisper is a general-purpose speech recognition model. It is trained on a large dataset of diverse audio and is also a multitasking model that can perform multilingual speech recognition, speech translation, and language identification.

tasks that can be performed using whisper:
* multilingual speech recognition
* speech translation
* spoken language identification
* voice activity detection

---


## Objective:

The objective of this task was to test out whisper for quran recitation and calculate the WER for several quran reciters.

---


## Experiment setup:

* Accelerator used for the experiment: GPU T4 x 2 (GPU Memory: 14.8 GB)

* All calculations were done on Juz 28 (from sorah 58 to sorah 66).

---


## Methodology:

### Datasets:
  * 3 different Quranic Recitation Packs were used from [[1](https://quran.ksu.edu.sa/ayat/?pg=patches&l=en)]
    (the pack provides separate `MP3` for each ayah).

    - Husary
    - Hudhaify
    - Minshawy Murattal

### Metric used: WER (Word Error Rate):
  $$ WER = \frac{(I + D + S)}{N} $$

  * `I`: the number of words inserted.
  * `D`: the number of words deleted.
  * `S`: the number of words substituted.
  * `N`: the total number of words in the reference.

### Approach:
  * Load the model and make it transcribe each ayah.

  * Clean the output text by striping the diacritic marks from the text.

  * Then compare the output text with the reference text using WER metric, this produces WER per ayah.

  * Save WER per ayah and `N` the number of words in the reference text per ayah so that we can merge multiple WER into one.

  * Calculating the WER for the whole sorah:
  
  $$I_{ayah} + D_{ayah} + S_{ayah} = WER_{ayah} * N_{ayah}$$

  $$I_{sorah} + D_{sorah} + S_{sorah} = \sum (I_{ayah} + D_{ayah} + S_{ayah})$$

  $$N_{sorah} = \sum N_{ayah}$$

  $$WER_{sorah} = \frac {\sum (I_{ayah} + D_{ayah} + S_{ayah})}{\sum N_{ayah}}$$

  therefore:
    $$WER_{sorah} = \frac {\sum (WER_{ayah} * N_{ayah})}{\sum N_{ayah}}$$

  * To calculate the WER for the whole juz the same approach is followed.


---


## Results:
  * Whisper's WER (`large-v2`) for general arabic is `16.00%` using `Fleurs dataset` according to the repo [[2](https://raw.githubusercontent.com/openai/whisper/main/language-breakdown.svg)].

  * for Quran Recitation:

    * Using the `small` model:

      | Reciter          | WER %  |
      |------------------|--------|
      | Husary-64kbps    | 57.82% |
      | Hudhaify-64kbps  | 25.96% |
      | Minshawy-128kbps | 30.69% |
      | Total            | 38.16% |

      ---

    * Using the `medium` model:

      | Reciter          | WER %  |
      |------------------|--------|
      | Husary-64kbps    | 19.74% |
      | Hudhaify-64kbps  | 17.85% |
      | Minshawy-128kbps | 16.29% |
      | Total            | 17.96% |

      ---
    
    * Using the `large` model:

      | Reciter          | WER %  |
      |------------------|--------|
      | Husary-64kbps    | 10.38% |
      | Hudhaify-64kbps  | 6.67%  |
      | Minshawy-128kbps | 18.08% |
      | Total            | 11.71% |

---


## Benchmarks:
### Notes:
- MP3 duration: the duration of audio in seconds.
- processing time: the time it took the model to produce output in seconds.
- this metric was measured for each ayah and those are the total numbers for the whole juz.

---

  * Using the `small` model:

    | Reciter          | MP3 duration (sec) | processing time (sec) |
    |------------------|--------------------|-----------------------|
    | Husary-64kbps    | 5114.38            | 253.20                |
    | Hudhaify-64kbps  | 3856.29            | 211.56                |
    | Minshawy-128kbps | 3325.44            | 212.23                |

    - GPU Memory Usage: 2.8 GB
    - total run time was `1014.5s` (`00:16:55`)

    ---

  * Using the `medium` model:

    | Reciter          | MP3 duration (sec) | processing time (sec) |
    |------------------|--------------------|-----------------------|
    | Husary-64kbps    | 5114.38            | 442.38                |
    | Hudhaify-64kbps  | 3856.29            | 395.85                |
    | Minshawy-128kbps | 3325.44            | 382.42                |

    - GPU Memory usage: 5.8 GB
    - total run time was `1634.7s` (`00:27:15`)

    ---

  * Using the `large` model:

    | Reciter          | MP3 duration (sec) | processing time (sec) |
    |------------------|--------------------|-----------------------|
    | Husary-64kbps    | 5114.38            | 578.13                |
    | Hudhaify-64kbps  | 3856.29            | 570.39                |
    | Minshawy-128kbps | 3325.44            | 768.94                |

    - GPU Memory usage: 10.7 GB 
    - total run time was `2351.5s` (`00:39:11`)

---


## Issues:
  * The output of some ayat had diacritic marks. (solution: strip the corresponding unicode code points before calculating WER).
  * Some parts are repeated, this affects the WER as they are counted as insertions. (we are still working on it).

---


## Comparisons:

According to [[3](https://doi.org/10.1007/s10772-022-09988-3)]:
  * They got 0.406 (40.6%) WER when testing by males on the male model they constructed using the same paramters used in the Mozilla's DeepSpeech project for the English model [[4](https://github.com/mozilla/DeepSpeech)].

---


## Datasets:
  * audio: https://www.kaggle.com/datasets/omartariq612/last-three-juz
  * reference text: https://www.kaggle.com/datasets/omartariq612/last-three-juz-text


---


## Source code:
  * notebook: https://www.kaggle.com/omartariq612/expirement-1
  * cli: https://github.com/OmarTariq612/whisper-quran-cli

---


## References:

[1] Ayat Recitation Packs: https://quran.ksu.edu.sa/ayat/?pg=patches&l=en

[2] openai/whisper github repo: https://github.com/openai/whisper

[3] Al-Issa, S., Al-Ayyoub, M., Al-Khaleel, O. et al. Building a neural speech recognizer for quranic recitations. Int J Speech Technol (2022). https://doi.org/10.1007/s10772-022-09988-3

[4] Mozilla's DeepSpeech implementation repo: https://github.com/mozilla/DeepSpeech