# We have 9 initial milestones:

- [ ] A model capable of detecting word level errors in realtime, regardless of changes in tempo and pitch, running on an average accelerator.
- [ ] A dataset with at least 100 Qara', the whole Holly Quran segmented into <=30s segments with their reference text.
- [ ] Start user testing when we get the `WER` below `5%`
- [ ] maybe a paper?
- [ ] Make that model deployable on average mobile devices
- [ ] A model capable of of detecting Tajwid level errors in realtime running on an average accelerator
- [ ] Make that model deployable on average mobile devices
- [ ] The undergrad thesis
- [ ] The final paper

I believe we will end up with more steps in-between but we will figure it as we go إن شاء الله.

# What we have tried so far:

- [01A](./01A.md): using vanilla whisper.
- [02A](./02A.md): Fine-tune Whisper (small) on a subset of the Quran.
- [02B](./02B.md): Segment the recitation audio files using whisper.
- [03A](./03A.md): investigate the effect of times stretching on the `WER` for both vanilla and fine-tuned models.
- [03B](./03B.md): Prototype an algorithm that fixes the reference text for the audio segments generated by in [02B](./02B.md).
- [04A](./04A.md)<sup>1</sup>: Make the model temp invariant by fine-tuning it with augmented data
- [user testing UI](https://colab.research.google.com/drive/13unB0wLetzKkS3wBgbETdT-oz9eNa192?usp=sharing): A UI that has the best model (always kept up-to-date) so far, used for user testing.

# Notes

1. Staring form [04A](./04A.md) we switched to using [insanely-fast-whisper](https://github.com/Vaibhavs10/insanely-fast-whisper).
