# Experiment 3: Time Stretching

_Sat Nov 18 01:05:54 AM EET 2023_

Author: Omar Tarek

Signed-off: Abdulrhmn Ghanem


## Objective

The objective of this task was to study the effects of time stretching on the results of whisper (vanilla and finetuned).

---


## Expirement setup

  * Accelerator used for the expirement: GPU T4 x 2

  * Effects were studied on:
    - Minshawy_Murattal_128kbps
    - Yasser_Ad-Dussary_128kbps

---


## Methodology

### Approach

* `audiomentations` [[1](https://github.com/iver56/audiomentations)] is a library that is mainly focused on audio data augmentation, It runs on CPU (`torch-audiomentations` [[2](https://github.com/asteroid-team/torch-audiomentations)]  is an alternative that runs on CPU and GPU  but currently it does not have the `TimeStretch` effect as it's in an early development stage).

* to load the audio file there are two ways:
  * using `torchaudio.load` function:
    * It's for generic use.
    * It returns a tensor of shape [channel, time] and the sample rate (if resampling is needed then use `torchaudio.transforms.Resample`).
    * you can choose the backend (sox_io, soundfile).

  * using `whisper.load_audio` [[3](https://github.com/openai/whisper/blob/main/whisper/audio.py#L25)] function:
    * It's specific to whisper, it reads audio files as mono `waveform` (down-mixing), resamping is done if necessary.
    * It returns a 1D numpy array.
    * Currently it spawns an ffmpeg process (it's much slower than `torchaudio.load`).

* To limit the variations, I've used `whisper.load_audio`, as it's being used by default if you call `model.transcribe` directly with a path to an audio file.

* Then pass the waveform to an instance of `TimeStretch`:

  ```python
  import whisper
  from audiomentations import TimeStretch

  PLAYBACK_SPEED = 0.85
  SAMPLE_RATE = 16000

  augment = TimeStretch(min_rate=PLAYBACK_SPEED,
                        max_rate=PLAYBACK_SPEED,
                        leave_length_unchanged=False,
                        p=1)
  audio_waveform = whisper.load_audio("/path/to/audio")
  augmented_waveform = augment(audio_waveform, sample_rate=SAMPLE_RATE)
  ```

* After that you can pass the `augmented_waveform` to the `transcribe` function.

---


## Results

| playback speed (reciter)            | vanilla-small WER % | vanilla-medium WER % | epoch-9 WER % |
|-------------------------------------|---------------------|----------------------|---------------|
| normal (Minshawy_Murattal_128kbps)  |       36.33%        |         20.70%       |     14.60%    |
| normal (Yasser_Ad-Dussary_128kbps)  |       25.87%        |         8.43%        |     11.75%    |
| 1.15x (Minshawy_Murattal_128kbps)   |       62.13%        |         36.78%       |     39.26%    |
| 1.15x (Yasser_Ad-Dussary_128kbps)   |       54.95%        |         29.96%       |     36.76%    |
| 0.85x (Minshawy_Murattal_128kbps)   |       60.41%        |         32.53%       |     37.00%    |
| 0.85x (Yasser_Ad-Dussary_128kbps)   |       46.74%        |         23.39%       |     31.69%    |

---

## Notes

  *  As mensioned the whisper paper[[4](https://arxiv.org/pdf/2212.04356.pdf)] in 2.4. Training Details:

      ```
      we do not use any data augmentation or regularization
      and instead rely on the diversity contained within such a large dataset
      to encourage generalization and robustness.
      ```

---


## Source code

  * whisper-small-1.15 notebook: https://www.kaggle.com/omartariq612/stretch-1-15-small

  * whisper-small-0.85 notebook: https://www.kaggle.com/omartariq612/stretch-0-85-small

  * whisper-medium-1.15 notebook: https://www.kaggle.com/code/omartariiik/stretch-1-15-medium

  * whisper-medium-0.85 notebook: https://www.kaggle.com/code/omartariiik/stretch-0-85-medium

  * whisper-epoch-9-1.15 notebook: https://www.kaggle.com/code/omartarik612/stretch-1-15-epoch-9

  * whisper-epoch-9-0.85 notebook: https://www.kaggle.com/code/omartarik612/stretch-0-85-epoch-9

  * whisper-quran-cli: https://github.com/OmarTariq612/whisper-quran-cli

---


## References

[1] audiomentations github repo: https://github.com/iver56/audiomentations

[2] torch-audiomentations github repo: https://github.com/asteroid-team/torch-audiomentations

[3] whisper load_audio function: https://github.com/openai/whisper/blob/main/whisper/audio.py#L25

[4] Robust Speech Recognition via Large-Scale Weak Supervision: https://arxiv.org/pdf/2212.04356.pdf
